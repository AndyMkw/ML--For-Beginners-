# 一个更现实的世界

在我们的情况下，彼得几乎可以一直走动而不会感到疲倦或饥饿。在一个更现实的世界里，我们要时不时地坐下来休息，也要吃东西。让我们通过实施以下规则让我们的世界更加现实：

1. 从一个地方搬到另一个地方，彼得失去了**能量**并获得了一些**疲劳**。
2. 彼得可以通过吃苹果来获得更多的能量。
3. 彼得可以通过在树下或草地上休息来消除疲劳（即走进有树和草的棋盘位置——绿色的格子）
4. 彼得需要找到并杀死狼
5. 为了杀死狼，彼得需要有一定的精力和疲劳，否则他会输掉这场战斗。

## 说明

使用原始 [notebook.ipynb](../notebook.ipynb) 笔记本作为解决方案的起点。

根据游戏规则修改上面的奖励函数，运行强化学习算法来学习赢得游戏的最佳策略，并将随机走动的结果与你的算法在游戏赢和输的数量方面进行比较。

> **注意**：在你的新世界中，状态更加复杂，除了人体位置还包括疲劳和能量水平。您可以选择将状态表示为一个元组（Board、energy、fatigue），或者为状态定义一个类（您可能还想从 `Board` 派生它），甚至在 [rlboard.py](../rlboard.py)中修改`Board`的源码。

在您的解决方案中，请保留负责随机走动策略的代码，并在最后将您的算法结果与随机走动进行比较。

> **注意**：您可能需要调整超参数才能使其工作，尤其是 epoch 数。因为游戏的成功（与狼搏斗）是一个罕见的事件，你需要期待更长的训练时间。

## 评判标准

| 标准 | 优秀 | 中规中矩 | 仍需努力 |
| -------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------ |
|          |笔记本上有新世界规则的定义、Q-Learning 算法和一些文字解释。与随机游走相比，Q-Learning 能够显着改善结果 | 介绍了 Notebook，实施了 Q-Learning 并与随机游走相比提高了结果，但不显着；或者 notebook 的文档不完善，代码结构不合理 |  一些试图重新定义世界规则的尝试，但 Q-Learning 算法不起作用，或者奖励函数没有完全定义 |
